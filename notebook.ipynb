{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferracioli/GabrielsMind/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmKTip2JOeEd"
      },
      "source": [
        "# Trabalho De ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LcGn6s_bOawi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# teste\n",
        "# teste 2\n",
        "\n",
        "import plotly.express as px\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error as RMSE\n",
        "\n",
        "from pathlib import Path\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faX4OUlFbSNi"
      },
      "source": [
        "# Redução do uso da memória"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMpx4whrjNr1"
      },
      "source": [
        "Devido ao consumo de memória do nosso dataset, decidimos aplicar algumas estratégias para a redução do uso pelo Pandas.\n",
        "Primeiro, mudamos o tipo de dado utilizado pelas colunas para formatos que ocupam menos bytes e transformamos o arquivo para o formato *.parquet, que tem melhor suporte à compressão de dados. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLDhQx3AtAWF"
      },
      "outputs": [],
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ1FqdNpjSwM"
      },
      "source": [
        "# Leitura dos arquivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHNyqFcOOdJz",
        "outputId": "0a8fca38-620a-46b1-efea-ed12a5c49380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content//drive\n"
          ]
        }
      ],
      "source": [
        "path = Path(\"/content/drive/MyDrive/datasets/dados-enem/\")\n",
        "drive.mount('/content//drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Stj3HCcV5Od"
      },
      "source": [
        "## Anotações:\n",
        "* Testar: se vale a pena eliminar quem está ausente plotando o gráfico pra ver a nota desse grupo de pessoas\n",
        "* Como fazer a conexão do jupyther com o SSH\n",
        "* https://python.plainenglish.io/how-to-create-a-interative-map-using-plotly-express-geojson-to-brazil-in-python-fb5527ae38fc\n",
        "\n",
        "## 1) Tratar dados\n",
        "* EDA Inicial\n",
        "* Tratar nulos (lembre-se de discutir e avaliar as melhores estratégias)\n",
        "* Mapear os valores e OneHotEncoding \n",
        "\n",
        "## 2) Preprocessamento\n",
        "* Remover colunas (correlacionadas [>80%], baixa variância, semântica)\n",
        "* (Opcional) Aplicar PCA \n",
        "* (Opcional) Feature Engineering\n",
        "* Standardize/Normalize\n",
        "* Tratar dados desbalanceados\n",
        "\n",
        "## 3) Modelo\n",
        "* Regressão linear<br>\n",
        "a. Realizar análise dos pesos<br>\n",
        "b. Aplicar técnicas de regularização<br> \n",
        "\n",
        "* Árvore de Decisão <br>\n",
        "a. Profundidade <br>\n",
        "b. Avaliar os cortes (impureza de gini / entropia) <br>\n",
        "\n",
        "* Naive Bayes <br>\n",
        "a. Quais features afetam significativamente P(nota|feature)<br>\n",
        "b. GaussianNaiveBayes x BernoulliNaiveBayes<br>\n",
        "\n",
        "* SVM<br>\n",
        "a. Avaliar o hiperplano gerado/ onde o corte é realizado <br>\n",
        "b. avaliar diferentes kernels <br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "\n",
        "\n",
        "  _algorithms = {\n",
        "      \n",
        "      'ElasticNet': {\n",
        "          'estimator':ElasticNet(),\n",
        "          'parameters':{\n",
        "              'alpha':[0.001, 0.5, 1.0],\n",
        "              'l1_ratio': [0, 0.5, 1.0]\n",
        "          }},\n",
        "\n",
        "      'DecisionTree': {\n",
        "          'estimator':DecisionTreeRegressor(),\n",
        "          'parameters':{\n",
        "              'max_depth':[None, 100, 90, 80, 70],\n",
        "              'min_samples_leaf':[1, 10, 20, 50, 100]\n",
        "          }},\n",
        "\n",
        "      'RandomForest': {\n",
        "          'estimator':RandomForestRegressor(),\n",
        "          'parameters':{\n",
        "              'n_estimators':[11, 31, 51],\n",
        "              'max_depth':[None, 100, 90, 80, 70],\n",
        "              'min_samples_leaf':[1, 10, 20, 50, 100],\n",
        "          }},\n",
        "\n",
        "      'KNN': {\n",
        "          'estimator':KNeighborsRegressor(),\n",
        "          'parameters':{\n",
        "              'n_neighbors':[5, 23, 47, 83],\n",
        "              'weights':['uniform', 'distance']\n",
        "          }},\n",
        "\n",
        "      'SVM': {\n",
        "          'estimator':SVR(),\n",
        "          'parameters':{\n",
        "              'kernel':['rbf', 'sigmoid'],\n",
        "              'gamma':[0.01, 0.5, 1.0],\n",
        "              'C':[10, 100, 1000]\n",
        "          }}\n",
        "\n",
        "  }\n",
        "\n",
        "  def __init__(self, verbose=True):\n",
        "    pass\n",
        "\n",
        "  def load(self, path, verbose=True):\n",
        "\n",
        "    self.train_df = pd.read_parquet(path/'train.parquet')\n",
        "    self.test_df = pd.read_parquet(path/'test.parquet') \n",
        "\n",
        "    if verbose:\n",
        "      print(\"Quantidade inicial de elementos no treino:\", len(self.train_df))\n",
        "      print(\"Quantidade inicial de elementos no teste:\", len(self.test_df))\n",
        "        \n",
        "    self.train_df.set_index(\"NU_INSCRICAO\", inplace=True)\n",
        "    self.test_df.set_index(\"NU_INSCRICAO\", inplace=True)\n",
        "\n",
        "    self._targets = [col for col in self.train_df.columns if \"NU_NOTA\" in col]\n",
        "\n",
        "\n",
        "\n",
        "  def prepare(self,verbose=True):\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Mapeando valores...\")    \n",
        "    self._map_values(verbose)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Criando novas colunas...\")\n",
        "    self._create_features(verbose)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Eliminando colunas...\")\n",
        "    self._clear_cols(verbose)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Aplicando get dummies...\")\n",
        "    self._create_dummies(verbose)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Selecionando features mais importantes\")\n",
        "    self._feature_selection(verbose)\n",
        "\n",
        "  def tune(self, random_state=0, verbose=True):\n",
        "\n",
        "\n",
        "    X, Y = self.train_df.drop(columns=self._targets), self.train_df[self._targets] \n",
        "\n",
        "    self._results = {}\n",
        "\n",
        "    for name, algorithm in self._algorithms.items():\n",
        "      if verbose:\n",
        "        print(name)\n",
        "\n",
        "      self._results = {}\n",
        "\n",
        "      for target in self._targets:\n",
        "        \n",
        "        model = GridSearchCV(algorithm['estimator'], algorithm['parameters'], verbose = 3, scoring='neg_root_mean_squared_error')\n",
        "        model.fit(X, Y[target])\n",
        "\n",
        "      break\n",
        "\n",
        "\n",
        "  def correlation(self, save=False, plot=True):\n",
        "    \n",
        "    fig = px.imshow(self.train_df.corr())\n",
        "    \n",
        "    if plot:\n",
        "      fig.show()\n",
        "\n",
        "    if save:\n",
        "      pass\n",
        "\n",
        "\n",
        "  def plot(self, column):\n",
        "    \n",
        "    tmp = self.train_df[column].value_counts()\n",
        "    fig = px.bar(x=tmp.index, y=tmp.values)\n",
        "    fig.show()\n",
        "    \n",
        "    melted = pd.melt(self.train_df, id_vars=[column], value_vars=self._targets, var_name='TP_NOTA', value_name='NU_NOTA')\n",
        "    fig=px.box(melted.sample(1000000), x='TP_NOTA', y='NU_NOTA', color=column)\n",
        "    fig.show()\n",
        "\n",
        "  def null_analysis(self, plot=True, save=False, verbose=True):\n",
        "    \n",
        "    null_count = self.train_df.isna().apply(np.sum, axis=0)/self.train_df.shape[0]\n",
        "    null_percentage_train = (null_count.loc[null_count!=0]*100).sort_values()\n",
        "    fig_train = px.bar(x=null_percentage_train.index, y=null_percentage_train.values, title=\"Porcentagem de valores nulos nos dados de treino\")\n",
        "\n",
        "    null_count = self.test_df.isna().apply(np.sum, axis=0)/self.test_df.shape[0]\n",
        "    null_percentage_test = (null_count.loc[null_count!=0]*100).sort_values()\n",
        "    fig_test = px.bar(x=null_percentage_test.index, y=null_percentage_test.values, title=\"Porcentagem de valores nulos nos dados de teste\")\n",
        "\n",
        "    if plot:\n",
        "      fig_train.show()\n",
        "      fig_test.show()\n",
        "\n",
        "    if save:\n",
        "      pass\n",
        "\n",
        "  def _feature_selection(self, verbose):\n",
        "    \n",
        "    to_drop = []\n",
        "    treshold = 0.05\n",
        "    for col in self.train_df.columns[1:]:\n",
        "       if self.train_df[col].std() < treshold:\n",
        "         to_drop.append(col)\n",
        "    \n",
        "    self.train_df.drop(columns=to_drop, inplace=True)\n",
        "    self.test_df.drop(columns=to_drop, inplace=True)\n",
        "    if verbose:\n",
        "      print(\"[VARIANCE TRESHOLD] Removendo colunas:\", to_drop)\n",
        "\n",
        "    #################################################################################\n",
        "\n",
        "    correlation = self.train_df.corr().abs()\n",
        "\n",
        "    upper_triangle = correlation.where(np.triu(np.ones(correlation.shape), k=1).astype(bool))\n",
        "\n",
        "    # Considera apenas colunas de correlação mínima de 0.85\n",
        "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
        "    \n",
        "    self.train_df.drop(columns=to_drop, inplace=True)\n",
        "    self.test_df.drop(columns=to_drop, inplace=True)\n",
        "\n",
        "    if verbose:\n",
        "      print('[HIGH CORRELATION] Eliminando colunas redundantes:', to_drop)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  def _clear_cols(self, verbose):\n",
        "    \n",
        "    null_count = self.train_df.isna().apply(np.sum, axis=0)/self.train_df.shape[0]\n",
        "    null_percentage_train = (null_count.loc[null_count!=0]*100).sort_values()\n",
        "\n",
        "    null_count = self.test_df.isna().apply(np.sum, axis=0)/self.test_df.shape[0]\n",
        "    null_percentage_test = (null_count.loc[null_count!=0]*100).sort_values()\n",
        "\n",
        "    to_drop_columns_train = list(null_percentage_train[null_percentage_train > 30].index)\n",
        "    to_drop_columns_test = list(null_percentage_test[null_percentage_test > 30].index)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"[NULLS] Colunas dropadas no treino:\", sorted(to_drop_columns_train))\n",
        "      print(\"[NULLS] Colunas dropadas no teste:\", sorted(to_drop_columns_test))\n",
        "\n",
        "    self.train_df.drop(columns=to_drop_columns_train, inplace=True)\n",
        "    self.test_df.drop(columns=to_drop_columns_test, inplace=True)\n",
        "\n",
        "    ###################################################################################################################\n",
        "\n",
        "    to_drop = ['CO_MUNICIPIO_RESIDENCIA', 'NO_MUNICIPIO_RESIDENCIA', 'CO_UF_RESIDENCIA', 'CO_MUNICIPIO_NASCIMENTO', 'NO_MUNICIPIO_NASCIMENTO',\n",
        "    'CO_UF_NASCIMENTO', 'SG_UF_NASCIMENTO', 'TP_ANO_CONCLUIU', 'IN_TREINEIRO', 'CO_MUNICIPIO_PROVA', 'NO_MUNICIPIO_PROVA', 'CO_UF_PROVA',\n",
        "    'SG_UF_PROVA']\n",
        "\n",
        "    self.train_df.drop(columns=to_drop, inplace=True)\n",
        "    self.test_df.drop(columns=to_drop, inplace=True)\n",
        "\n",
        "    if verbose:\n",
        "      print(f'[DROP COLUMNS] Colunas retiradas por falta de relevânica:{[to_drop]}')\n",
        "\n",
        "    ##################################################################################################################\n",
        "\n",
        "    to_drop = self.train_df[(self.train_df['TP_STATUS_REDACAO'].isna()) & (self.train_df['TP_PRESENCA_CH']=='Presente')].index\n",
        "    self.train_df.drop(to_drop, inplace=True)\n",
        "\n",
        "    to_drop = self.test_df[(self.test_df['TP_STATUS_REDACAO'].isna()) & (self.test_df['TP_PRESENCA_CH']=='Presente')].index\n",
        "    self.test_df.drop(to_drop, inplace=True)\n",
        "\n",
        "    if verbose:\n",
        "      print(f'[INCONSISTENCY] Removendo inconsistências.')\n",
        "\n",
        "    ##################################################################################################################\n",
        "    \n",
        "\n",
        "    to_drop = ['NU_NOTA_MT', 'NU_NOTA_CH', 'NU_NOTA_CN', 'NU_NOTA_LC', 'NU_NOTA_REDACAO', 'TP_STATUS_REDACAO']\n",
        "    self.train_df.dropna(subset=to_drop, inplace=True)\n",
        "\n",
        "    try:\n",
        "      self.test_df.dropna(subset=to_drop, inplace=True)\n",
        "    except KeyError:\n",
        "      pass #\n",
        "\n",
        "    if verbose:\n",
        "      print('[NULL TARGETS] Removendo valores nulos nas colunas-alvo')\n",
        "\n",
        "\n",
        "    #####################################################################################################################\n",
        "\n",
        "    self.train_df.drop(self.train_df[self.train_df['TP_STATUS_REDACAO'] != 'Sem problemas'].index, inplace=True)\n",
        "    self.test_df.drop(self.test_df[self.test_df['TP_STATUS_REDACAO'] != 'Sem problemas'].index, inplace=True)\n",
        "\n",
        "    if verbose:\n",
        "      print('[::] Removendo redações que tiraram nota 0')\n",
        "\n",
        "\n",
        "  def _create_dummies(self, verbose):\n",
        "\n",
        "    cols = [col for col in self.train_df.columns if ((self.train_df[col].dtype == 'object') or (self.train_df[col].dtype.name == 'category'))]\n",
        "\n",
        "    self.train_df = pd.get_dummies(self.train_df, columns=cols)\n",
        "    self.test_df = pd.get_dummies(self.test_df, columns=cols)\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"[GET DUMMIES] Colunas categóricas convertidas: {cols}\")\n",
        "\n",
        "\n",
        "  def _create_features(self, verbose):\n",
        "\n",
        "    new_columns = []\n",
        "    filled_columns = []\n",
        "    ############################################################################################\n",
        "\n",
        "    uf_regiao = {\n",
        "      'RR':'Norte', 'AP':'Norte', 'AM':'Norte', 'PA':'Norte', 'AC':'Norte', 'RO':'Norte', 'TO':'Norte', 'MA':'Nordeste',\n",
        "      'PI':'Nordeste', 'CE':'Nordeste', 'RN':'Nordeste', 'PB':'Nordeste', 'PE':'Nordeste', 'AL':'Nordeste', 'SE':'Nordeste',\n",
        "      'BA':'Nordeste', 'MT':'Centro-oeste', 'DF':'Centro-oeste', 'GO':'Centro-oeste', 'MS':'Centro-oeste', 'MG':'Sudeste',\n",
        "      'ES':'Sudeste', 'RJ':'Sudeste', 'SP':'Sudeste', 'PR':'Sul', 'SC':'Sul', 'RS':'Sul', \n",
        "      }\n",
        "\n",
        "    self.train_df['NO_REGIAO_RESIDENCIA'] = self.train_df['SG_UF_RESIDENCIA'].map(uf_regiao)\n",
        "    self.test_df['NO_REGIAO_RESIDENCIA'] = self.test_df['SG_UF_RESIDENCIA'].map(uf_regiao)\n",
        "\n",
        "    new_columns.append('NO_REGIAO_RESIDENCIA')\n",
        "\n",
        "    ############################################################################################\n",
        "\n",
        "    mean_score_per_reg = self.train_df.groupby(\"NO_REGIAO_RESIDENCIA\")[self._targets].mean()\n",
        "    for col in self._targets:\n",
        "      self.train_df[\"REG_NOTA_\"+col.split(\"_\")[2]+\"_MEDIA\"] = self.train_df['NO_REGIAO_RESIDENCIA'].apply(\n",
        "          lambda row: mean_score_per_reg[col][row]) \n",
        "      self.test_df[\"REG_NOTA_\"+col.split(\"_\")[2]+\"_MEDIA\"] = self.test_df['NO_REGIAO_RESIDENCIA'].apply(\n",
        "          lambda row: mean_score_per_reg[col][row]) \n",
        "\n",
        "      new_columns.append(\"REG_NOTA_\"+col.split(\"_\")[2]+\"_MEDIA\")\n",
        "    ############################################################################################\n",
        "  \n",
        "    \n",
        "    self.train_df['TP_MINORIA_RACIAL'] = ((self.train_df['TP_COR_RACA'] != 'Branca').astype(int) + (self.train_df['TP_COR_RACA'] != 'Amarela').astype(int)) -1\n",
        "    self.test_df['TP_MINORIA_RACIAL'] = ((self.test_df['TP_COR_RACA'] != 'Branca').astype(int) + (self.test_df['TP_COR_RACA'] != 'Amarela').astype(int)) -1\n",
        "\n",
        "    new_columns.append('TP_MINORIA_RACIAL')\n",
        "    ############################################################################################\n",
        "\n",
        "    cols = [col for col in self.train_df.columns if ((\"IN_\" in col) and ('TREINEIRO' not in col))]\n",
        "\n",
        "    self.train_df['TP_SITUACAO_ESPECIAL'] = self.train_df[cols].any(axis=1)\n",
        "    self.test_df['TP_SITUACAO_ESPECIAL'] = self.test_df[cols].any(axis=1)\n",
        "\n",
        "    new_columns.append('TP_SITUACAO_ESPECIAL')\n",
        "\n",
        "    #############################################################################################\n",
        "\n",
        "\n",
        "    self.train_df['TP_SOLTEIRO'] = self.train_df['TP_ESTADO_CIVIL'] == 'Solteiro(a)'\n",
        "    self.test_df['TP_SOLTEIRO'] = self.test_df['TP_ESTADO_CIVIL'] == 'Solteiro(a)'\n",
        "\n",
        "    new_columns.append('TP_SOLTEIRO')\n",
        "\n",
        "    #############################################################################################\n",
        "\n",
        "    median_train = self.train_df.loc[self.train_df['NU_IDADE'].notnull(), 'NU_IDADE'].median()\n",
        "    \n",
        "    self.train_df['NU_IDADE'] = self.train_df['NU_IDADE'].fillna(median_train)\n",
        "    self.test_df['NU_IDADE'] = self.test_df['NU_IDADE'].fillna(median_train)\n",
        "  \n",
        "    filled_columns.append(\"NU_IDADE\")\n",
        "    #############################################################################################\n",
        "\n",
        "    if verbose:\n",
        "      print(f'[FEATURE ENGINEERING] Novas colunas: {new_columns}')\n",
        "      print(f'[INPUTATION] Colunas com valores nulos preenchidos: {filled_columns}')\n",
        "      \n",
        "\n",
        "\n",
        "  \n",
        "  def _map_values(self, verbose):\n",
        "    #################################################################\n",
        "    rename = {0:\"0\",#np.NaN,\n",
        "      1:\"Solteiro(a)\",\n",
        "      2:\"Casado(a)/Mora com companheiro(a)\",\n",
        "      3:\"Divorciado(a)/Desquitado(a)/Separado(a)\",\n",
        "      4:\"Viúvo(a)\"}\n",
        "\n",
        "    self.train_df['TP_ESTADO_CIVIL'] = self.train_df['TP_ESTADO_CIVIL'].map(rename)\n",
        "    self.test_df['TP_ESTADO_CIVIL'] = self.test_df['TP_ESTADO_CIVIL'].map(rename)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {0:\"0\",#np.NaN,\n",
        "      1:\"Branca\",\n",
        "      2:\"Preta\",\n",
        "      3:\"Parda\",\n",
        "      4:\"Amarela\",\n",
        "      5:\"Indígena\"}\n",
        "\n",
        "    self.train_df['TP_COR_RACA'] = self.train_df['TP_COR_RACA'].map(rename)\n",
        "    self.test_df['TP_COR_RACA'] = self.test_df['TP_COR_RACA'].map(rename)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {0:\"0\",#np.NaN,\n",
        "      1:\"Brasileiro(a)\",\n",
        "      2:\"Brasileiro(a) Naturalizado(a)\",\n",
        "      3:\"Estrangeiro(a)\",\n",
        "      4:\"Brasileiro(a) Nato(a), nascido(a) no exterior\"\n",
        "      }\n",
        "\n",
        "    self.train_df['TP_NACIONALIDADE'] = self.train_df['TP_NACIONALIDADE'].map(rename)\n",
        "    self.test_df['TP_NACIONALIDADE'] = self.test_df['TP_NACIONALIDADE'].map(rename)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {1:\"Já concluí o Ensino Médio\",\n",
        "      2:\"Estou cursando e concluirei o Ensino Médio no ano corrente\",\n",
        "      3:\"Estou cursando e concluirei o Ensino Médio após o ano corrente\",\n",
        "      4:\"Não concluí e não estou cursando o Ensino Médio\"\n",
        "      }\n",
        "\n",
        "    self.train_df['TP_ST_CONCLUSAO'] = self.train_df['TP_ST_CONCLUSAO'].map(rename)\n",
        "    self.test_df['TP_ST_CONCLUSAO'] = self.test_df['TP_ST_CONCLUSAO'].map(rename)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {0:\"0\",#np.NaN,\n",
        "      1:\"2018\",\n",
        "      2:\"2017\",\n",
        "      3:\"2016\",\n",
        "      4:\"2015\",\n",
        "      5:\"2014\",\n",
        "      6:\"2013\",\n",
        "      7:\"2012\",\n",
        "      8:\"2011\",\n",
        "      9:\"2010\",\n",
        "      10:\"2009\",\n",
        "      11:\"2008\",\n",
        "      12:\"2007\",\n",
        "      13:\"Antes de 2007\"}\n",
        "\n",
        "    self.train_df['TP_ANO_CONCLUIU'] = self.train_df['TP_ANO_CONCLUIU'].map(rename)\n",
        "    self.test_df['TP_ANO_CONCLUIU'] = self.test_df['TP_ANO_CONCLUIU'].map(rename)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {1:\"0\",#np.NaN,\n",
        "      2:\"Pública\",\n",
        "      3:\"Privada\",\n",
        "      4:\"Exterior\"}\n",
        "\n",
        "    self.train_df['TP_ESCOLA'] = self.train_df['TP_ESCOLA'].map(rename)\n",
        "    self.test_df['TP_ESCOLA'] = self.test_df['TP_ESCOLA'].map(rename)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {1:\"Federal\",\n",
        "      2:\"Estadual\",\n",
        "      3:\"Municipal\",\n",
        "      4:\"Privada\"}\n",
        "\n",
        "    self.train_df['TP_DEPENDENCIA_ADM_ESC'] = self.train_df['TP_DEPENDENCIA_ADM_ESC'].map(rename)\n",
        "    self.test_df['TP_DEPENDENCIA_ADM_ESC'] = self.test_df['TP_DEPENDENCIA_ADM_ESC'].map(rename)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {1:\"Ensino Regular\",\n",
        "      2:\"Educação Especial - Modalidade Substitutiva\",\n",
        "      3:\"Educação de Jovens e Adultos\"}\n",
        "\n",
        "    self.train_df['TP_ENSINO'] = self.train_df['TP_ENSINO'].map(rename)\n",
        "    self.test_df['TP_ENSINO'] = self.test_df['TP_ENSINO'].map(rename)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {0:\"Ausente\",\n",
        "      1:\"Presente\",\n",
        "      2:\"Eliminado\"}\n",
        "\n",
        "    for c in [col for col in self.train_df.columns if \"TP_PRESENCA\" in col]:\n",
        "      self.train_df[c] = self.train_df[c].map(rename)\n",
        "      self.test_df[c] = self.test_df[c].map(rename)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        1:\"Sem problemas\",\n",
        "        2:\"Anulada\",\n",
        "        3:\"Copiou texto motivador\",\n",
        "        4:\"Em branco\",\n",
        "        6:\"Fuga ao tema\",\n",
        "        7:\"Não atende tipo textual\",\n",
        "        8:\"Texto insuficiente\",\n",
        "        9:\"Parte desconectada\"\n",
        "      }\n",
        "\n",
        "    self.train_df['TP_STATUS_REDACAO'] = self.train_df['TP_STATUS_REDACAO'].map(rename)\n",
        "    self.test_df['TP_STATUS_REDACAO'] = self.test_df['TP_STATUS_REDACAO'].map(rename)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5,\n",
        "        'F':6,\n",
        "        'G':7,\n",
        "        'H':0\n",
        "    }\n",
        "\n",
        "    self.train_df['Q001'] = self.train_df['Q001'].map(rename).astype(int)\n",
        "    self.test_df['Q001'] = self.test_df['Q001'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5,\n",
        "        'F':6,\n",
        "        'G':7,\n",
        "        'H':0\n",
        "    }\n",
        "\n",
        "    self.train_df['Q002'] = self.train_df['Q002'].map(rename).astype(int)\n",
        "    self.test_df['Q002'] = self.test_df['Q002'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5,\n",
        "        'F':0,\n",
        "    }\n",
        "\n",
        "    self.train_df['Q003'] = self.train_df['Q003'].map(rename).astype(int)\n",
        "    self.test_df['Q003'] = self.test_df['Q003'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5,\n",
        "        'F':0,\n",
        "    }\n",
        "\n",
        "    self.train_df['Q004'] = self.train_df['Q004'].map(rename).astype(int)\n",
        "    self.test_df['Q004'] = self.test_df['Q004'].map(rename).astype(int)\n",
        "\n",
        "    #Q005 já é numérica\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5,\n",
        "        'F':6,\n",
        "        'G':7,\n",
        "        'H':8,\n",
        "        'I':9,\n",
        "        'J':10,\n",
        "        'K':11,\n",
        "        'L':12,\n",
        "        'M':13,\n",
        "        'N':14,\n",
        "        'O':15,\n",
        "        'P':16,\n",
        "        'Q':17\n",
        "    }\n",
        "\n",
        "    self.train_df['Q006'] = self.train_df['Q006'].map(rename).astype(int)\n",
        "    self.test_df['Q006'] = self.test_df['Q006'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "    }\n",
        "\n",
        "    self.train_df['Q007'] = self.train_df['Q007'].map(rename).astype(int)\n",
        "    self.test_df['Q007'] = self.test_df['Q007'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q008'] = self.train_df['Q008'].map(rename).astype(int)\n",
        "    self.test_df['Q008'] = self.test_df['Q008'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q009'] = self.train_df['Q009'].map(rename).astype(int)\n",
        "    self.test_df['Q009'] = self.test_df['Q009'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q010'] = self.train_df['Q010'].map(rename).astype(int)\n",
        "    self.test_df['Q010'] = self.test_df['Q010'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q011'] = self.train_df['Q011'].map(rename).astype(int)\n",
        "    self.test_df['Q011'] = self.test_df['Q011'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q012'] = self.train_df['Q012'].map(rename).astype(int)\n",
        "    self.test_df['Q012'] = self.test_df['Q012'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q013'] = self.train_df['Q013'].map(rename).astype(int)\n",
        "    self.test_df['Q013'] = self.test_df['Q013'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q014'] = self.train_df['Q014'].map(rename).astype(int)\n",
        "    self.test_df['Q014'] = self.test_df['Q014'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q015'] = self.train_df['Q015'].map(rename).astype(int)\n",
        "    self.test_df['Q015'] = self.test_df['Q015'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q016'] = self.train_df['Q016'].map(rename).astype(int)\n",
        "    self.test_df['Q016'] = self.test_df['Q016'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q017'] = self.train_df['Q017'].map(rename).astype(int)\n",
        "    self.test_df['Q017'] = self.test_df['Q017'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':0,\n",
        "        'B':1,\n",
        "    }\n",
        "\n",
        "    self.train_df['Q018'] = self.train_df['Q018'].map(rename).astype(int)\n",
        "    self.test_df['Q018'] = self.test_df['Q018'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q019'] = self.train_df['Q019'].map(rename).astype(int)\n",
        "    self.test_df['Q019'] = self.test_df['Q019'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':0,\n",
        "        'B':1,\n",
        "    }\n",
        "\n",
        "    self.train_df['Q020'] = self.train_df['Q020'].map(rename).astype(int)\n",
        "    self.test_df['Q020'] = self.test_df['Q020'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':0,\n",
        "        'B':1,\n",
        "    }\n",
        "\n",
        "    self.train_df['Q021'] = self.train_df['Q021'].map(rename).astype(int)\n",
        "    self.test_df['Q021'] = self.test_df['Q021'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q022'] = self.train_df['Q022'].map(rename).astype(int)\n",
        "    self.test_df['Q022'] = self.test_df['Q022'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':0,\n",
        "        'B':1,\n",
        "    }\n",
        "\n",
        "    self.train_df['Q023'] = self.train_df['Q023'].map(rename).astype(int)\n",
        "    self.test_df['Q023'] = self.test_df['Q023'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':1,\n",
        "        'B':2,\n",
        "        'C':3,\n",
        "        'D':4,\n",
        "        'E':5\n",
        "    }\n",
        "\n",
        "    self.train_df['Q024'] = self.train_df['Q024'].map(rename).astype(int)\n",
        "    self.test_df['Q024'] = self.test_df['Q024'].map(rename).astype(int)\n",
        "\n",
        "    #################################################################\n",
        "    rename = {\n",
        "        'A':0,\n",
        "        'B':1,\n",
        "    }\n",
        "\n",
        "    self.train_df['Q025'] = self.train_df['Q025'].map(rename).astype(int)\n",
        "    self.test_df['Q025'] = self.test_df['Q025'].map(rename).astype(int)"
      ],
      "metadata": {
        "id": "monL-6iYH2lL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "model.load(path)"
      ],
      "metadata": {
        "id": "OdCHx9nx79Rt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424bb3ac-841a-4b75-8651-af992cd48cf4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade inicial de elementos no treino: 3311925\n",
            "Quantidade inicial de elementos no teste: 1783345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.prepare()"
      ],
      "metadata": {
        "id": "tRA3Key2O2af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d7d3b5-ba24-4ba3-d1fc-4f24d9b5d760"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapeando valores...\n",
            "Criando novas colunas...\n",
            "[FEATURE ENGINEERING] Novas colunas: ['NO_REGIAO_RESIDENCIA', 'REG_NOTA_CN_MEDIA', 'REG_NOTA_CH_MEDIA', 'REG_NOTA_LC_MEDIA', 'REG_NOTA_MT_MEDIA', 'REG_NOTA_REDACAO_MEDIA', 'TP_MINORIA_RACIAL', 'TP_SITUACAO_ESPECIAL', 'TP_SOLTEIRO']\n",
            "[INPUTATION] Colunas com valores nulos preenchidos: ['NU_IDADE']\n",
            "Eliminando colunas...\n",
            "[NULLS] Colunas dropadas no treino: ['CO_ESCOLA', 'CO_MUNICIPIO_ESC', 'CO_UF_ESC', 'NO_MUNICIPIO_ESC', 'SG_UF_ESC', 'TP_DEPENDENCIA_ADM_ESC', 'TP_ENSINO', 'TP_LOCALIZACAO_ESC', 'TP_SIT_FUNC_ESC']\n",
            "[NULLS] Colunas dropadas no teste: ['CO_ESCOLA', 'CO_MUNICIPIO_ESC', 'CO_UF_ESC', 'NO_MUNICIPIO_ESC', 'SG_UF_ESC', 'TP_DEPENDENCIA_ADM_ESC', 'TP_ENSINO', 'TP_LOCALIZACAO_ESC', 'TP_SIT_FUNC_ESC']\n",
            "[DROP COLUMNS] Colunas retiradas por falta de relevânica:[['CO_MUNICIPIO_RESIDENCIA', 'NO_MUNICIPIO_RESIDENCIA', 'CO_UF_RESIDENCIA', 'CO_MUNICIPIO_NASCIMENTO', 'NO_MUNICIPIO_NASCIMENTO', 'CO_UF_NASCIMENTO', 'SG_UF_NASCIMENTO', 'TP_ANO_CONCLUIU', 'IN_TREINEIRO', 'CO_MUNICIPIO_PROVA', 'NO_MUNICIPIO_PROVA', 'CO_UF_PROVA', 'SG_UF_PROVA']]\n",
            "[INCONSISTENCY] Removendo inconsistências.\n",
            "[NULL TARGETS] Removendo valores nulos nas colunas-alvo\n",
            "[::] Removendo redações que tiraram nota 0\n",
            "Aplicando get dummies...\n",
            "[GET DUMMIES] Colunas categóricas convertidas: ['SG_UF_RESIDENCIA', 'TP_SEXO', 'TP_ESTADO_CIVIL', 'TP_COR_RACA', 'TP_NACIONALIDADE', 'TP_ST_CONCLUSAO', 'TP_ESCOLA', 'TP_PRESENCA_CN', 'TP_PRESENCA_CH', 'TP_PRESENCA_LC', 'TP_PRESENCA_MT', 'TP_STATUS_REDACAO', 'NO_REGIAO_RESIDENCIA']\n",
            "Selecionando features mais importantes\n",
            "[VARIANCE TRESHOLD] Removendo colunas: ['IN_BAIXA_VISAO', 'IN_CEGUEIRA', 'IN_SURDEZ', 'IN_DEFICIENCIA_AUDITIVA', 'IN_SURDO_CEGUEIRA', 'IN_DEFICIENCIA_FISICA', 'IN_DEFICIENCIA_MENTAL', 'IN_DEFICIT_ATENCAO', 'IN_DISLEXIA', 'IN_DISCALCULIA', 'IN_AUTISMO', 'IN_VISAO_MONOCULAR', 'IN_OUTRA_DEF', 'IN_GESTANTE', 'IN_LACTANTE', 'IN_IDOSO', 'IN_ESTUDA_CLASSE_HOSPITALAR', 'IN_SEM_RECURSO', 'IN_BRAILLE', 'IN_AMPLIADA_24', 'IN_AMPLIADA_18', 'IN_LEDOR', 'IN_ACESSO', 'IN_TRANSCRICAO', 'IN_LIBRAS', 'IN_LEITURA_LABIAL', 'IN_MESA_CADEIRA_RODAS', 'IN_MESA_CADEIRA_SEPARADA', 'IN_APOIO_PERNA', 'IN_GUIA_INTERPRETE', 'IN_COMPUTADOR', 'IN_CADEIRA_ESPECIAL', 'IN_CADEIRA_CANHOTO', 'IN_CADEIRA_ACOLCHOADA', 'IN_PROVA_DEITADO', 'IN_MOBILIARIO_OBESO', 'IN_LAMINA_OVERLAY', 'IN_PROTETOR_AURICULAR', 'IN_MEDIDOR_GLICOSE', 'IN_MAQUINA_BRAILE', 'IN_SOROBAN', 'IN_MARCA_PASSO', 'IN_SONDA', 'IN_MEDICAMENTOS', 'IN_SALA_INDIVIDUAL', 'IN_SALA_ESPECIAL', 'IN_SALA_ACOMPANHANTE', 'IN_MOBILIARIO_ESPECIFICO', 'IN_MATERIAL_ESPECIFICO', 'IN_NOME_SOCIAL', 'SG_UF_RESIDENCIA_RR', 'TP_ESTADO_CIVIL_Viúvo(a)', 'TP_NACIONALIDADE_0', 'TP_NACIONALIDADE_Brasileiro(a) Nato(a), nascido(a) no exterior', 'TP_NACIONALIDADE_Estrangeiro(a)', 'TP_PRESENCA_CN_Presente', 'TP_PRESENCA_CH_Presente', 'TP_PRESENCA_LC_Presente', 'TP_PRESENCA_MT_Presente', 'TP_STATUS_REDACAO_Sem problemas']\n",
            "[HIGH CORRELATION] Eliminando colunas redundantes: ['REG_NOTA_CH_MEDIA', 'REG_NOTA_LC_MEDIA', 'REG_NOTA_MT_MEDIA', 'REG_NOTA_REDACAO_MEDIA', 'TP_SEXO_M', 'TP_ESTADO_CIVIL_Solteiro(a)', 'TP_COR_RACA_Branca', 'TP_NACIONALIDADE_Brasileiro(a) Naturalizado(a)', 'TP_ESCOLA_0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.tune()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cdr-0BdVPAbq",
        "outputId": "3001c09f-e99a-4fd0-fe8b-0c3de9b6d674"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet\n",
            "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=1;, score=-92.198 total time= 1.2min\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=1;, score=-92.050 total time= 1.2min\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=1;, score=-91.871 total time= 1.2min\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=1;, score=-92.093 total time= 1.2min\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=1;, score=-92.087 total time= 1.2min\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=10;, score=-71.106 total time=  57.8s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=10;, score=-70.966 total time=  56.9s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=10;, score=-71.042 total time=  56.8s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=10;, score=-70.965 total time= 1.1min\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=10;, score=-70.968 total time=  56.6s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=20;, score=-67.750 total time=  53.5s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=20;, score=-67.637 total time=  59.3s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=20;, score=-67.686 total time=  58.9s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=20;, score=-67.590 total time=  59.0s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=20;, score=-67.580 total time=  53.3s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=50;, score=-65.589 total time=  52.7s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=50;, score=-65.525 total time=  48.7s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=50;, score=-65.556 total time=  48.7s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=50;, score=-65.470 total time=  48.2s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=50;, score=-65.516 total time=  48.5s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=100;, score=-64.909 total time=  48.3s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=100;, score=-64.836 total time=  42.5s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=100;, score=-64.861 total time=  42.3s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=100;, score=-64.815 total time=  41.9s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=100;, score=-64.803 total time=  42.4s\n",
            "[CV 1/5] END max_depth=100, min_samples_leaf=1;, score=-92.166 total time= 1.1min\n",
            "[CV 2/5] END max_depth=100, min_samples_leaf=1;, score=-91.969 total time= 1.3min\n",
            "[CV 3/5] END max_depth=100, min_samples_leaf=1;, score=-91.878 total time= 1.3min\n",
            "[CV 4/5] END max_depth=100, min_samples_leaf=1;, score=-92.089 total time= 1.1min\n",
            "[CV 5/5] END max_depth=100, min_samples_leaf=1;, score=-92.158 total time= 1.1min\n",
            "[CV 1/5] END max_depth=100, min_samples_leaf=10;, score=-71.105 total time=  53.0s\n",
            "[CV 2/5] END max_depth=100, min_samples_leaf=10;, score=-70.967 total time=  53.1s\n",
            "[CV 3/5] END max_depth=100, min_samples_leaf=10;, score=-71.042 total time=  53.0s\n",
            "[CV 4/5] END max_depth=100, min_samples_leaf=10;, score=-70.963 total time=  59.8s\n",
            "[CV 5/5] END max_depth=100, min_samples_leaf=10;, score=-70.968 total time=  59.6s\n",
            "[CV 1/5] END max_depth=100, min_samples_leaf=20;, score=-67.750 total time=  55.1s\n",
            "[CV 2/5] END max_depth=100, min_samples_leaf=20;, score=-67.637 total time=  49.9s\n",
            "[CV 3/5] END max_depth=100, min_samples_leaf=20;, score=-67.685 total time=  55.3s\n",
            "[CV 4/5] END max_depth=100, min_samples_leaf=20;, score=-67.590 total time=  49.5s\n",
            "[CV 5/5] END max_depth=100, min_samples_leaf=20;, score=-67.579 total time=  55.2s\n",
            "[CV 1/5] END max_depth=100, min_samples_leaf=50;, score=-65.589 total time=  49.1s\n",
            "[CV 2/5] END max_depth=100, min_samples_leaf=50;, score=-65.525 total time=  45.5s\n",
            "[CV 3/5] END max_depth=100, min_samples_leaf=50;, score=-65.556 total time=  48.2s\n",
            "[CV 4/5] END max_depth=100, min_samples_leaf=50;, score=-65.470 total time=  45.7s\n",
            "[CV 5/5] END max_depth=100, min_samples_leaf=50;, score=-65.516 total time=  45.4s\n",
            "[CV 1/5] END max_depth=100, min_samples_leaf=100;, score=-64.909 total time=  47.2s\n",
            "[CV 2/5] END max_depth=100, min_samples_leaf=100;, score=-64.836 total time=  46.9s\n",
            "[CV 3/5] END max_depth=100, min_samples_leaf=100;, score=-64.861 total time=  41.2s\n",
            "[CV 4/5] END max_depth=100, min_samples_leaf=100;, score=-64.815 total time=  40.5s\n",
            "[CV 5/5] END max_depth=100, min_samples_leaf=100;, score=-64.802 total time=  39.7s\n",
            "[CV 1/5] END max_depth=90, min_samples_leaf=1;, score=-92.095 total time=  56.8s\n",
            "[CV 2/5] END max_depth=90, min_samples_leaf=1;, score=-92.007 total time=  55.6s\n",
            "[CV 3/5] END max_depth=90, min_samples_leaf=1;, score=-91.836 total time= 1.3min\n",
            "[CV 4/5] END max_depth=90, min_samples_leaf=1;, score=-92.006 total time= 1.3min\n",
            "[CV 5/5] END max_depth=90, min_samples_leaf=1;, score=-92.106 total time=  54.4s\n",
            "[CV 1/5] END max_depth=90, min_samples_leaf=10;, score=-71.107 total time=  43.7s\n",
            "[CV 2/5] END max_depth=90, min_samples_leaf=10;, score=-70.966 total time=  43.6s\n",
            "[CV 3/5] END max_depth=90, min_samples_leaf=10;, score=-71.042 total time=  43.7s\n",
            "[CV 4/5] END max_depth=90, min_samples_leaf=10;, score=-70.965 total time=  57.1s\n",
            "[CV 5/5] END max_depth=90, min_samples_leaf=10;, score=-70.968 total time=  43.7s\n",
            "[CV 1/5] END max_depth=90, min_samples_leaf=20;, score=-67.749 total time=  41.8s\n",
            "[CV 2/5] END max_depth=90, min_samples_leaf=20;, score=-67.637 total time=  41.9s\n",
            "[CV 3/5] END max_depth=90, min_samples_leaf=20;, score=-67.686 total time=  41.8s\n",
            "[CV 4/5] END max_depth=90, min_samples_leaf=20;, score=-67.590 total time=  41.9s\n",
            "[CV 5/5] END max_depth=90, min_samples_leaf=20;, score=-67.579 total time=  52.4s\n",
            "[CV 1/5] END max_depth=90, min_samples_leaf=50;, score=-65.589 total time=  47.4s\n",
            "[CV 2/5] END max_depth=90, min_samples_leaf=50;, score=-65.525 total time=  39.5s\n",
            "[CV 3/5] END max_depth=90, min_samples_leaf=50;, score=-65.556 total time=  47.5s\n",
            "[CV 4/5] END max_depth=90, min_samples_leaf=50;, score=-65.470 total time=  47.6s\n",
            "[CV 5/5] END max_depth=90, min_samples_leaf=50;, score=-65.515 total time=  47.5s\n",
            "[CV 1/5] END max_depth=90, min_samples_leaf=100;, score=-64.909 total time=  37.6s\n",
            "[CV 2/5] END max_depth=90, min_samples_leaf=100;, score=-64.835 total time=  43.9s\n",
            "[CV 3/5] END max_depth=90, min_samples_leaf=100;, score=-64.861 total time=  43.8s\n",
            "[CV 4/5] END max_depth=90, min_samples_leaf=100;, score=-64.815 total time=  43.8s\n",
            "[CV 5/5] END max_depth=90, min_samples_leaf=100;, score=-64.802 total time=  44.0s\n",
            "[CV 1/5] END max_depth=80, min_samples_leaf=1;, score=-92.169 total time= 1.2min\n",
            "[CV 2/5] END max_depth=80, min_samples_leaf=1;, score=-91.981 total time=  54.3s\n",
            "[CV 3/5] END max_depth=80, min_samples_leaf=1;, score=-91.903 total time=  54.1s\n",
            "[CV 4/5] END max_depth=80, min_samples_leaf=1;, score=-92.129 total time=  54.0s\n",
            "[CV 5/5] END max_depth=80, min_samples_leaf=1;, score=-92.071 total time=  54.0s\n",
            "[CV 1/5] END max_depth=80, min_samples_leaf=10;, score=-71.107 total time=  43.3s\n",
            "[CV 2/5] END max_depth=80, min_samples_leaf=10;, score=-70.967 total time=  56.1s\n",
            "[CV 3/5] END max_depth=80, min_samples_leaf=10;, score=-71.042 total time=  43.4s\n",
            "[CV 4/5] END max_depth=80, min_samples_leaf=10;, score=-70.964 total time=  56.4s\n",
            "[CV 5/5] END max_depth=80, min_samples_leaf=10;, score=-70.968 total time=  56.5s\n",
            "[CV 1/5] END max_depth=80, min_samples_leaf=20;, score=-67.749 total time=  41.7s\n",
            "[CV 2/5] END max_depth=80, min_samples_leaf=20;, score=-67.637 total time=  53.0s\n",
            "[CV 3/5] END max_depth=80, min_samples_leaf=20;, score=-67.686 total time=  41.7s\n",
            "[CV 4/5] END max_depth=80, min_samples_leaf=20;, score=-67.590 total time=  52.9s\n",
            "[CV 5/5] END max_depth=80, min_samples_leaf=20;, score=-67.579 total time=  52.7s\n",
            "[CV 1/5] END max_depth=80, min_samples_leaf=50;, score=-65.589 total time=  39.3s\n",
            "[CV 2/5] END max_depth=80, min_samples_leaf=50;, score=-65.525 total time=  47.9s\n",
            "[CV 3/5] END max_depth=80, min_samples_leaf=50;, score=-65.556 total time=  47.8s\n",
            "[CV 4/5] END max_depth=80, min_samples_leaf=50;, score=-65.470 total time=  39.3s\n",
            "[CV 5/5] END max_depth=80, min_samples_leaf=50;, score=-65.516 total time=  47.6s\n",
            "[CV 1/5] END max_depth=80, min_samples_leaf=100;, score=-64.909 total time=  43.7s\n",
            "[CV 2/5] END max_depth=80, min_samples_leaf=100;, score=-64.835 total time=  44.2s\n",
            "[CV 3/5] END max_depth=80, min_samples_leaf=100;, score=-64.861 total time=  37.6s\n",
            "[CV 4/5] END max_depth=80, min_samples_leaf=100;, score=-64.815 total time=  37.5s\n",
            "[CV 5/5] END max_depth=80, min_samples_leaf=100;, score=-64.803 total time=  44.0s\n",
            "[CV 1/5] END max_depth=70, min_samples_leaf=1;, score=-92.185 total time=  54.4s\n",
            "[CV 2/5] END max_depth=70, min_samples_leaf=1;, score=-92.088 total time=  54.6s\n",
            "[CV 3/5] END max_depth=70, min_samples_leaf=1;, score=-91.947 total time= 1.2min\n",
            "[CV 4/5] END max_depth=70, min_samples_leaf=1;, score=-92.045 total time= 1.2min\n",
            "[CV 5/5] END max_depth=70, min_samples_leaf=1;, score=-92.130 total time=  54.3s\n",
            "[CV 1/5] END max_depth=70, min_samples_leaf=10;, score=-71.107 total time=  43.7s\n",
            "[CV 2/5] END max_depth=70, min_samples_leaf=10;, score=-70.965 total time=  56.9s\n",
            "[CV 3/5] END max_depth=70, min_samples_leaf=10;, score=-71.041 total time=  43.6s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0ce1b539b905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-95dde0e10839>\u001b[0m in \u001b[0;36mtune\u001b[0;34m(self, random_state, verbose)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'estimator'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_root_mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         )\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VA4TF8HEvafe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ML-Entrega2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}